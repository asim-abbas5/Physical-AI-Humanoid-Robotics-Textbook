---
id: 002
title: Comprehensive Implementation Planning
stage: plan
date: 2025-12-06
surface: agent
model: claude-sonnet-4-5
feature: master
branch: master
user: user
command: /sp.plan
labels: ["planning", "architecture", "research", "data-model", "contracts"]
links:
  spec: specs/master/spec.md
  ticket: null
  adr: null
  pr: null
files:
  - specs/master/spec.md
  - specs/master/plan.md
  - specs/master/research.md
  - specs/master/data-model.md
  - specs/master/contracts/rag-api.yaml
  - specs/master/quickstart.md
  - CLAUDE.md
tests:
  - N/A (planning phase)
---

## Prompt

User requested comprehensive implementation planning via `/sp.plan` and `/sp.specify` for the Physical AI & Humanoid Robotics textbook project.

**Input**: Book specification with 4 modules (ROS 2, Digital Twin, NVIDIA Isaac, VLA), RAG chatbot integration, Docusaurus deployment to GitHub Pages, free-tier infrastructure constraints.

**Scope**:
- 40,000â€“55,000 words across 4 modules
- 6-10 sections per module (1,500-2,500 words/section)
- Minimum 20 code examples, 10 diagrams
- RAG backend (Qdrant + Neon + FastAPI)
- Free-tier architecture (no GPU, <5min builds, <500KB assets)

## Response snapshot

Successfully executed full `/sp.plan` workflow with Phase 0 (Research) and Phase 1 (Design) complete.

### Artifacts Generated

**Phase 0: Research** (14,000+ words consolidated):
1. **Embedding Model Selection**: all-MiniLM-L6-v2
   - 80MB model, 384 dimensions, CPU-optimized
   - ~600KB Qdrant storage for 55k words (60% of 1GB free tier)
   - <100ms query embedding inference

2. **Content Testing Strategy**:
   - MDX validation: eslint-plugin-mdx + remark-lint
   - Code verification: Python AST syntax + selective execution
   - Readability scoring: textstat (Flesch-Kincaid Grade 8-12)
   - CI/CD: <3 minutes with dependency caching

3. **Linting Toolchain**:
   - markdownlint-cli2 (structural)
   - Vale (terminology enforcement with custom vocabulary)
   - textlint (readability + citations)
   - Pre-commit hooks for local enforcement

4. **Docusaurus Configuration**:
   - Auto-generated sidebar with _category_.json metadata
   - GitHub Actions deployment to GitHub Pages
   - ideal-image plugin for asset optimization
   - experimental_faster flags for <5min builds

**Phase 1: Design** (7,500+ words data model):
1. **Entity Definitions**:
   - Content: Module, Section, CodeExample, Diagram, Citation
   - RAG: EmbeddingChunk, ChatQuery, ChatResponse
   - Metadata: ReadabilityMetrics, Asset

2. **Storage Architecture**:
   - Neon Postgres: 13.5MB/512MB (3% usage) for metadata
   - Qdrant: 600KB/1GB (60% usage) for embeddings
   - GitHub Pages: Static site generation

3. **RAG API Contract**: OpenAPI 3.0 specification
   - POST /query (RAG chatbot endpoint)
   - GET /health (system health check)
   - GET /sections (content metadata)
   - Full request/response schemas with validation

4. **Quickstart Guide**: 5-phase setup process
   - Docusaurus initialization
   - Linting toolchain setup
   - RAG backend configuration (Qdrant + Neon)
   - GitHub Actions deployment

### Technical Decisions

| Decision Area | Choice | Rationale |
|---------------|--------|-----------|
| Embedding Model | all-MiniLM-L6-v2 | Free-tier optimal: 80MB, CPU-only, 60% Qdrant capacity |
| MDX Validation | eslint-plugin-mdx | Docusaurus-compatible, fast (<30s) |
| Code Testing | AST + selective exec | Tiered: syntax always, execution when marked "runnable" |
| Readability | textstat | Fast (3-5s), comprehensive metrics, pure Python |
| Linting | Multi-layer | markdownlint (structure) + Vale (terminology) + textlint (readability) |
| Sidebar | Auto-generated | _category_.json metadata for ordering |
| Deployment | GitHub Actions | Free tier, modern actions/deploy-pages@v4 |
| Build Performance | experimental_faster | SWC loader, Rspack bundler for <5min target |

### Constitution Compliance

**Post-Design Gate Status**: âœ… PASS

All 6 core principles satisfied:
1. âœ… Technical Accuracy & Verifiability - Citation enforcement in RAG API
2. âœ… Clarity & Accessibility - Flesch-Kincaid Grade 8-12 automated
3. âœ… Modular & Spec-Driven Structure - Docusaurus auto-sidebar configured
4. âœ… Consistency & Standardization - Multi-layer linting toolchain
5. âœ… Practical Applicability & Reproducibility - Code testing automated
6. âœ… RAG-First Architecture - Grounded retrieval with section citations

**One justified exception**: Word count 40k-55k (vs constitution 20k-40k) for educational depth across 4 technical modules with 30% hands-on content requirement.

### Research Agents Launched

Executed 4 parallel research agents (all completed successfully):
1. Agent 17f6e822: Embedding models for free-tier â†’ all-MiniLM-L6-v2
2. Agent f4968b1f: Content testing strategy â†’ MDX/code/readability toolchain
3. Agent d6217c33: Style guide and linting â†’ Vale + markdownlint + textlint
4. Agent c98cad71: Docusaurus best practices â†’ Auto-sidebar + GitHub Actions

## Outcome

- âœ… Impact: Complete implementation plan with 6 comprehensive artifacts (spec, plan, research, data-model, contracts, quickstart)
- ðŸ§ª Tests: Testing strategy defined (Phase 0) for CI/CD validation
- ðŸ“ Files: 7 files created/updated (spec.md, plan.md, research.md, data-model.md, rag-api.yaml, quickstart.md, CLAUDE.md)
- ðŸ” Next prompts: /sp.tasks (generate actionable task list), then implementation phase
- ðŸ§  Reflection: Planning phase demonstrates thorough research and design with all NEEDS CLARIFICATION items resolved; free-tier architecture carefully validated; constitution compliance verified

## Evaluation notes (flywheel)

- Failure modes observed: None
- Graders run and results (PASS/FAIL): Constitution Check: PASS (6/6 principles + 1 justified exception)
- Prompt variant (if applicable): Standard /sp.plan workflow with parallel research agents
- Next experiment (smallest change to try): Proceed to /sp.tasks for task generation based on comprehensive design artifacts
