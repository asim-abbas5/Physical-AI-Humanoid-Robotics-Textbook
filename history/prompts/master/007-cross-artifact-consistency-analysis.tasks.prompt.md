---
id: 007
title: Cross-artifact consistency analysis
stage: tasks
date: 2025-12-06
surface: agent
model: claude-sonnet-4-5
feature: master
branch: master
user: developer
command: /sp.analyze
labels: ["analysis", "consistency", "validation", "quality-assurance"]
links:
  spec: specs/master/spec.md
  ticket: null
  adr: null
  pr: null
files:
  - specs/master/spec.md (read)
  - specs/master/plan.md (read)
  - specs/master/tasks.md (read)
  - specs/master/data-model.md (read)
  - specs/master/contracts/rag-api.yaml (read)
  - .specify/memory/constitution.md (read)
tests:
  - Constitution alignment check across all principles
  - Requirements coverage verification (18/18 requirements mapped)
  - Cross-artifact consistency validation
  - Terminology drift detection
  - Ambiguity and underspecification detection
---

## Prompt

/sp.analyze

## Response snapshot

Performed comprehensive cross-artifact analysis of spec.md, plan.md, tasks.md, data-model.md, contracts/rag-api.yaml, and constitution.md for the Physical AI & Humanoid Robotics textbook project.

**Key Findings**:
- **12 total issues** identified across 6 categories
- **1 CRITICAL** constitution violation (word count) - already justified in plan.md
- **5 HIGH-priority** issues requiring resolution before implementation:
  - I1: Module count inconsistency (spec: 4 modules, plan: "6 chapters")
  - I2: Section word count range conflict (1,000-3,000 vs 1,500-2,500)
  - I3: Query max length not enforced in ChatWidget tasks
  - A1: Ambiguous word count "limit" definition
  - U2: Missing max selection length validation in UI tasks
- **100% requirements coverage**: All 18 functional requirements mapped to tasks
- **Constitution compliance**: All principles pass except justified word count exception

**Analysis Methodology**:
1. Loaded artifacts progressively (spec, plan, tasks, data-model, contracts, constitution)
2. Built semantic models (requirements inventory, user story mapping, task coverage)
3. Executed detection passes (duplication, ambiguity, underspecification, constitution alignment, coverage gaps, inconsistency)
4. Assigned severity (CRITICAL/HIGH/MEDIUM/LOW) based on impact on implementation
5. Generated findings table with specific locations and recommendations

**Output Format**: Markdown analysis report with:
- Executive summary
- Findings table (12 rows)
- Coverage summary table (18 requirements)
- Constitution alignment matrix
- Metrics dashboard
- Next actions with priorities

## Outcome

- ‚úÖ Impact: Identified 5 HIGH-priority issues blocking implementation readiness; provided actionable remediation roadmap
- üß™ Tests: Validated 100% requirements coverage; detected 3 validation gaps in implementation tasks
- üìÅ Files: Read 6 artifacts (spec, plan, tasks, data-model, API contract, constitution); no modifications (read-only analysis)
- üîÅ Next prompts: Resolve HIGH-priority issues (I1, I2, I3, A1, U2) then re-run /sp.analyze to verify clean slate before /sp.implement
- üß† Reflection: Progressive disclosure strategy kept token usage efficient (~52k tokens); semantic modeling enabled cross-artifact inconsistency detection; constitution as non-negotiable authority clarified severity assignment

## Evaluation notes (flywheel)

- Failure modes observed: None; analysis completed successfully with actionable findings
- Graders run and results (PASS/FAIL):
  - Constitution Check: PASS (1 justified exception documented)
  - Requirements Coverage: PASS (100%, 18/18)
  - Task Format Validation: PASS (all tasks follow checklist format)
  - Cross-artifact Consistency: FAIL (5 HIGH-priority issues detected)
- Prompt variant (if applicable): N/A (standard /sp.analyze execution)
- Next experiment (smallest change to try): Implement auto-remediation for LOW/MEDIUM issues (terminology standardization, validation script additions) to reduce manual cleanup burden
